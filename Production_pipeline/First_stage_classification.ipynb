{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "3Gb4r0Yd3fhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxcTGksJ3Ywn"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install xformers\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jsonlines\n",
        "! pip install fast_ml --quiet\n",
        "! pip install transformers\n",
        "! pip install nltk\n",
        "! python -m nltk.downloader all\n",
        "! pip install unidecode"
      ],
      "metadata": {
        "id": "2AXzHuTV3inl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "import nltk\n",
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoModel, AutoModelForSequenceClassification,AutoTokenizer,pipeline"
      ],
      "metadata": {
        "id": "sMS4kqXj3lRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and transformation"
      ],
      "metadata": {
        "id": "JdlM2cZ43ovS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_total = pd.read_parquet('/content/drive/MyDrive/Dissertation/Data/df.parquet')\n",
        "# One-shot sample\n",
        "df_label_unique_sample = df_total.groupby('label_cat', group_keys=False).apply(lambda df: df.sample(1))\n",
        "# Create label mapping\n",
        "label = list(df_label_unique_sample['label'])\n",
        "id = list(df_label_unique_sample['label_cat'])\n",
        "label_to_id = dict(zip(label,id))\n",
        "id_to_label = dict(zip(id,label))"
      ],
      "metadata": {
        "id": "nCrZFn1z3nL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions"
      ],
      "metadata": {
        "id": "byNnYoEs3zWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_neo = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Dissertation/GPT_data/GPT_Neo_model/augmented_gptneo_100\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# Create a pipeline\n",
        "classifier_neo = pipeline(\"text-classification\", model=model_neo,tokenizer=tokenizer_bert)"
      ],
      "metadata": {
        "id": "tZoYQ0sA3s4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that performs text classification\n",
        "def open_file(path):\n",
        "  with open(path) as f:\n",
        "    lines = f.readlines()\n",
        "    # Remove all \\n only elements\n",
        "    lines = [i for i in lines if i != '\\n']\n",
        "    # Remove all lines that is less than 15 characters which is the new line or section\n",
        "    lines = [i for i in lines if len(i)>15]\n",
        "    # Remove \\n for each elemtn in the text file\n",
        "    lines = [i[:-1] for i in lines]\n",
        "  return lines"
      ],
      "metadata": {
        "id": "PbNgyiaB3u1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset for word embedding\n",
        "def pre_process(text):\n",
        "    # convert input corpus to lower case.\n",
        "    text = text.lower()\n",
        "    # collecting a list of stop words from nltk and punctuation form\n",
        "    # string class and create single array.\n",
        "    stopset = stopwords.words('english') + list(string.punctuation)\n",
        "    # remove stop words and punctuations from string.\n",
        "    # word_tokenize is used to tokenize the input corpus in word tokens.\n",
        "    text = \" \".join([i for i in word_tokenize(text) if i not in stopset])\n",
        "    return text\n",
        "\n",
        "# Lemmatization and spell check\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Word Tokenisationb & Lemmatization\n",
        "def word_token(sentence):\n",
        "  words = word_tokenize(sentence)\n",
        "  for w in range(0,len(words)):\n",
        "    words[w]=lemmatizer.lemmatize(words[w])\n",
        "    w+=1\n",
        "  words = ' '.join(words)\n",
        "  return words\n",
        "\n",
        "#Pass the processed text into the pipeline\n",
        "def modelling(lines,classifier):\n",
        "  processed_lines = []\n",
        "  for i in range(len(lines)):\n",
        "    lines_processed = pre_process(lines[i])\n",
        "    line_token = word_token(lines_processed)\n",
        "    result = classifier(line_token )[0]\n",
        "    result['Text'] = line_token\n",
        "    processed_lines.append(result)\n",
        "    i+=1\n",
        "  return processed_lines"
      ],
      "metadata": {
        "id": "027bc8DP36un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find consecutive lines of text with the same labelling\n",
        "def consecutive_check(file):\n",
        "  consecutive_check = []\n",
        "  for i in range(0,len(file)-1):\n",
        "    curr_label = list(file[i].values())[0]\n",
        "    next_label = list(file[i+1].values())[0]\n",
        "    if curr_label == next_label:\n",
        "      consecutive_check.append(1)\n",
        "    else:\n",
        "      consecutive_check.append(0)\n",
        "    i+=1\n",
        "  return consecutive_check\n",
        "\n",
        "# Find index of those consecutive elements\n",
        "def consecutive_check_index(consecutive_check):\n",
        "  consecutive_check_index = []\n",
        "  for i in range(len(consecutive_check)):\n",
        "    if consecutive_check[i]==1:\n",
        "      consecutive_check_index.append(i)\n",
        "  return consecutive_check_index\n",
        "\n",
        "# Find the index of consecutive elements stops\n",
        "def non_consecutive_check_index(consecutive_check_index):\n",
        "  non_consecutive_check_index = []\n",
        "  for i in range(0,len(consecutive_check_index)-1):\n",
        "    if consecutive_check_index[i+1] - consecutive_check_index[i]!=1:\n",
        "      non_consecutive_check_index.append(i+1) # Adjust the end index\n",
        "  return non_consecutive_check_index\n",
        "\n",
        "\n",
        "# Insert starting index\n",
        "def segment(non_consecutive_check_index1,consecutive_check_index):\n",
        "  segment_consecutive_check = []\n",
        "  for i in range(len(non_consecutive_check_index1)-1):\n",
        "    curr = non_consecutive_check_index1[i]\n",
        "    next = non_consecutive_check_index1[i+1]\n",
        "    list_consecutive = consecutive_check_index[curr:next]\n",
        "    segment_consecutive_check.append(list_consecutive)\n",
        "    max_indices = max(segment_consecutive_check[i])\n",
        "    mapping_indices = [max_indices+1, max_indices+2]\n",
        "    for x in mapping_indices:\n",
        "      segment_consecutive_check[i].append(x)\n",
        "    i+=1\n",
        "  return segment_consecutive_check\n",
        "\n",
        "def text_concat(segment_consecutive_check,pre_processed_files):\n",
        "  # Text Concatenation dictionary\n",
        "  for i in range(len(segment_consecutive_check)):\n",
        "    dic={}\n",
        "    list_dir = segment_consecutive_check[i]\n",
        "    min_dir = min(list_dir)\n",
        "    max_dir = max(list_dir) # Adjust for mapping\n",
        "    List_text = pre_processed_files[min_dir : max_dir]\n",
        "    Text = ' '.join([d.get('Text') for d in List_text])\n",
        "    Label = pre_processed_files[min_dir]['label']\n",
        "    Score = max([d.get('score') for d in List_text]) # Use the average score\n",
        "    dic['Text'] = Text\n",
        "    dic['label'] = Label\n",
        "    dic['score'] = Score\n",
        "    pre_processed_files.append(dic)\n",
        "    i+=1\n",
        "\n",
        "def removal(segment_consecutive_check,pre_processed_files):\n",
        "  # remove original individual text from extraction\n",
        "  segment_consecutive_check_reverse = sorted(segment_consecutive_check, reverse=True)\n",
        "  for i in range(len(segment_consecutive_check_reverse)):\n",
        "    list_dir = segment_consecutive_check[i]\n",
        "    min_dir = min(list_dir)\n",
        "    max_dir = max(list_dir)\n",
        "    del pre_processed_files[min_dir:max_dir]\n",
        "    i+=1\n",
        "\n",
        "def concatenate_original(lines,segment_consecutive_check): # Concatenate processed lines\n",
        "  for i in range(len(segment_consecutive_check)):\n",
        "    list_dir = segment_consecutive_check[i]\n",
        "    min_dir = min(list_dir)\n",
        "    max_dir = max(list_dir) # Adjust for mapping\n",
        "    List_text = lines[min_dir : max_dir]\n",
        "    Text = ' '.join([str(item) for item in List_text])\n",
        "    lines.append(Text)\n",
        "    i+=1\n",
        "  removal(segment_consecutive_check,lines)\n",
        "  return lines"
      ],
      "metadata": {
        "id": "VNSTHM7M3921"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the best results for each label and find unprocessed text in original file. All has been stored into one pandas dataframe\n",
        "def result_to_table(procssed_list,orig_list):\n",
        "  # Maximum score of each label\n",
        "  label = [d.get('label') for d in procssed_list]\n",
        "  text = [d.get('Text') for d in procssed_list]\n",
        "  score = [d.get('score') for d in procssed_list]\n",
        "  data = {'Text': text, 'Label':label , 'score': score}\n",
        "  # Get the largest score of sample with the same label\n",
        "  sample = pd.DataFrame(data)\n",
        "  # Add an index column\n",
        "  sample['Index_ref'] = sample.index\n",
        "  max_table = sample[sample.groupby('Label')['score'].transform(max) == sample['score']]\n",
        "  max_table.sort_values(by=['Index_ref'],inplace=True)\n",
        "  # Get the +-2 sentences for each label\n",
        "  index_list = sorted(list(max_table['Index_ref']))\n",
        "  #return index_list\n",
        "  text_all= []\n",
        "  label_all = []\n",
        "  for i in index_list:\n",
        "    text_range = orig_list[i-3:i+3]\n",
        "    text_all.append(text_range)\n",
        "  max_table['Orig_Text'] = text_all\n",
        "  max_table['Label_cat'] = max_table['Label'].map(label_to_id)\n",
        "  max_table.drop(columns = ['Index_ref','Text'],axis=1,inplace=True)\n",
        "  # Remove Duplicate label\n",
        "  max_table_final = max_table.drop_duplicates(subset=['Label','Label_cat'], keep='first')"
      ],
      "metadata": {
        "id": "WGN7SGs-4BQF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}